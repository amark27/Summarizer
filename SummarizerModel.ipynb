{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "\n",
    "def tfidf(corpus):\n",
    "    '''\n",
    "    Computes the TF-IDF (term frequency - inverse document frequency) matrix\n",
    "\n",
    "    Args\n",
    "    - corpus: a list of documents\n",
    "\n",
    "    Returns\n",
    "    - tfidfVec: an m x n matrix of the corpus. m = number of documents, n = number of different terms used in the documents\n",
    "    '''\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidfVec = vectorizer.fit_transform(corpus)\n",
    "    return tfidfVec\n",
    "\n",
    "def svd(tfidfVec):\n",
    "    '''\n",
    "    Gives the singular value decomposition of an m x n matrix.\n",
    "    A = U * sigma * V^t\n",
    "    \n",
    "    Args\n",
    "    - tfidfVec: an m x n matrix. m = number of documents or sentences, n = number of terms\n",
    "\n",
    "    Returns\n",
    "    - u: an m x r matrix of left singular values (document-topic table). r = number of topics\n",
    "    - sigma: an r x r diagonal matrix of singular values in decreasing order across the diagonal\n",
    "    - V^t: an n x r matrix of right singular values (term-topic table)\n",
    "    '''\n",
    "\n",
    "    lsa = TruncatedSVD(n_components=10, n_iter=50)\n",
    "    u = lsa.fit_transform(tfidfVec)\n",
    "    sigma = lsa.singular_values_\n",
    "    vt = lsa.components_.T\n",
    "\n",
    "    return u, sigma, vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 4)\t0.26084107346995766\n  (0, 14)\t0.26084107346995766\n  (0, 26)\t0.1540569399192421\n  (0, 5)\t0.26084107346995766\n  (0, 19)\t0.26084107346995766\n  (0, 29)\t0.26084107346995766\n  (0, 1)\t0.26084107346995766\n  (0, 13)\t0.1540569399192421\n  (0, 30)\t0.26084107346995766\n  (0, 33)\t0.5216821469399153\n  (0, 6)\t0.26084107346995766\n  (0, 16)\t0.26084107346995766\n  (0, 15)\t0.26084107346995766\n  (1, 2)\t0.20542217148685335\n  (1, 31)\t0.20542217148685335\n  (1, 9)\t0.20542217148685335\n  (1, 24)\t0.20542217148685335\n  (1, 8)\t0.20542217148685335\n  (1, 32)\t0.20542217148685335\n  (1, 0)\t0.20542217148685335\n  (1, 23)\t0.20542217148685335\n  (1, 11)\t0.20542217148685335\n  (1, 21)\t0.20542217148685335\n  (1, 20)\t0.20542217148685335\n  (1, 25)\t0.20542217148685335\n  (1, 17)\t0.20542217148685335\n  (1, 10)\t0.20542217148685335\n  (1, 27)\t0.20542217148685335\n  (1, 34)\t0.4108443429737067\n  (1, 12)\t0.4108443429737067\n  (1, 26)\t0.1213256436566357\n  (1, 13)\t0.1213256436566357\n  (2, 3)\t0.4189401020758947\n  (2, 18)\t0.4189401020758947\n  (2, 7)\t0.4189401020758947\n  (2, 22)\t0.4189401020758947\n  (2, 28)\t0.4189401020758947\n  (2, 26)\t0.24743277305481848\n  (2, 13)\t0.24743277305481848\n----------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "     topic1    topic2    topic3\n0  0.610244 -0.566882 -0.553396\n1  0.539674  0.792453 -0.284201\n2  0.673459 -0.121359  0.729194",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic1</th>\n      <th>topic2</th>\n      <th>topic3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.610244</td>\n      <td>-0.566882</td>\n      <td>-0.553396</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.539674</td>\n      <td>0.792453</td>\n      <td>-0.284201</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.673459</td>\n      <td>-0.121359</td>\n      <td>0.729194</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------\n[1.05697402 0.9818676  0.95851027]\n----------------------------------------------------------\n[[ 0.09923171  0.16885535 -0.06354471]\n [ 0.14247906 -0.15337777 -0.15711534]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.25254266 -0.05273725  0.33250769]\n [ 0.14247906 -0.15337777 -0.15711534]\n [ 0.14247906 -0.15337777 -0.15711534]\n [ 0.14247906 -0.15337777 -0.15711534]\n [ 0.25254266 -0.05273725  0.33250769]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.19846343  0.33771071 -0.12708942]\n [ 0.29191404 -0.02200616  0.06605902]\n [ 0.14247906 -0.15337777 -0.15711534]\n [ 0.14247906 -0.15337777 -0.15711534]\n [ 0.14247906 -0.15337777 -0.15711534]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.25254266 -0.05273725  0.33250769]\n [ 0.14247906 -0.15337777 -0.15711534]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.25254266 -0.05273725  0.33250769]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.29191404 -0.02200616  0.06605902]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.25254266 -0.05273725  0.33250769]\n [ 0.14247906 -0.15337777 -0.15711534]\n [ 0.14247906 -0.15337777 -0.15711534]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.09923171  0.16885535 -0.06354471]\n [ 0.28495812 -0.30675554 -0.31423067]\n [ 0.19846343  0.33771071 -0.12708942]]\n----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'In this paper, we propose two generic text summarization methods that create text summaries by ranking and extracting sentences from the original documents.',\n",
    "    'The first method uses standard IR methods to rank sentence relevances, while the second method uses the latent semantic analysis technique to identify semantically important sentences, for summary creations.',\n",
    "    'Both methods strive to select sentences that are highly ranked and different from each other.'\n",
    "]\n",
    "\n",
    "tfidfVec = tfidf(corpus)\n",
    "print(tfidfVec)\n",
    "print('----------------------------------------------------------')\n",
    "svdVec, sigma, vt = svd(tfidfVec)\n",
    "\n",
    "df = pd.DataFrame(svdVec, columns=[f'topic{str(i)}' for i in range(1, svdVec.shape[1]+1)])\n",
    "\n",
    "display(df)\n",
    "print('----------------------------------------------------------')\n",
    "print(sigma)\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "print(vt.T)\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "\n",
    "\n",
    "#df = pd.DataFrame()\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizerEnv",
   "language": "python",
   "name": "summarizerenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}