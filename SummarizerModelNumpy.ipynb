{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer Model with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "#download assets from nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def tfidf(corpus):\n",
    "    '''\n",
    "    Computes the TF-IDF (term frequency - inverse document frequency) matrix\n",
    "\n",
    "    Args\n",
    "    - corpus: a list of sentences (documents) that need to be summarized (m x n matrix)\n",
    "    m = number of different terms used in the documents, n = number of documents (not 0)\n",
    "\n",
    "    Returns\n",
    "    - tfidf_vec: an m x n matrix of the corpus\n",
    "    - vocab: all the unique words used in the corpus, excluding stop words\n",
    "\n",
    "    https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html\n",
    "    '''\n",
    "    # calculate term frequency matrix\n",
    "    num_docs = len(corpus)\n",
    "    stop_words = stopwords.words('english')\n",
    "    word_sentence = []\n",
    "    vocab = set()\n",
    "\n",
    "    # sanitize text and break up each sentence into individual words\n",
    "    for doc in corpus:\n",
    "        #sanitize_text = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "        sanitize_text = doc\n",
    "        tokenized = [word.lower() for word in word_tokenize(sanitize_text)]\n",
    "        tokenized = [word for word in tokenized if word not in stop_words and word not in string.punctuation]\n",
    "        word_sentence.append(tokenized)    \n",
    "        vocab = vocab.union(set(tokenized))\n",
    "    \n",
    "    word_ind = {word : i for i, word in enumerate(vocab)}\n",
    "    tf = np.zeros((len(vocab), num_docs))\n",
    "\n",
    "    for i, words in enumerate(word_sentence):\n",
    "        for word in words:\n",
    "            tf[word_ind[word], i] += 1\n",
    "    \n",
    "    dft = np.sum(np.greater(tf, [0]).astype(float), axis=1)\n",
    "    idf = np.log(np.divide([num_docs], dft))\n",
    "    tfidf_vec= tf * np.expand_dims(idf, axis=1)\n",
    "\n",
    "    return tfidf_vec, vocab\n",
    "\n",
    "def svd(doc_term_matrix):\n",
    "    '''\n",
    "    Gives the singular value decomposition of an m x n matrix.\n",
    "    A = U * sigma * V^t\n",
    "    \n",
    "    Args\n",
    "    - doc_term_matrix: an m x n matrix. m = number of documents or sentences, n = number of terms\n",
    "\n",
    "    Returns\n",
    "    - u: an m x r matrix of left singular values (document-topic table). r = number of topics\n",
    "    - sigma: an r x r diagonal matrix of singular values in decreasing order across the diagonal\n",
    "    - v_t: an n x r matrix of right singular values (term-topic table)\n",
    "    '''\n",
    "\n",
    "    lsa = TruncatedSVD(n_components = 10, n_iter=20)\n",
    "    u = lsa.fit_transform(doc_term_matrix)\n",
    "    sigma = lsa.singular_values_\n",
    "    v_t = lsa.components_.T\n",
    "\n",
    "    return u, sigma, v_t\n",
    "\n",
    "def weigh_sentence_importance(u, sigma):\n",
    "    '''\n",
    "    Uses the LSA enhancement described by Josef Steinberg, et al. to weigh\n",
    "    sentence importance from topics\n",
    "    Takes all topics that have singular values > half of the largest singular value\n",
    "\n",
    "    Compute s_k = sqrt(sum(v_ki^2 * sigma_i^2) from i = 1 to n) for all sentences\n",
    "    s_k is the length of the vector of the kth sentence\n",
    "    n is the number of topics \n",
    "\n",
    "    Args\n",
    "    - U, sigma matrices from SVD\n",
    "\n",
    "    Returns\n",
    "    - Vector of each sentence weight as calculated above (1 x m)\n",
    "    '''\n",
    "\n",
    "    #look for the sigma value range that we need to consider using binary search\n",
    "    #sigma array is sorted in descending order and will never be empty\n",
    "    l, r, target = 0, len(sigma), sigma[0]/2\n",
    "    while l < r:\n",
    "        mid = l + (r-l)//2\n",
    "\n",
    "        if sigma[mid] < target:\n",
    "            r = mid\n",
    "        else:\n",
    "            l = mid + 1\n",
    "    sigma_bound = l\n",
    "\n",
    "    u_slice = u[:, :sigma_bound]\n",
    "    sigma_slice = sigma[:sigma_bound]\n",
    "    u_sq = np.square(u_slice)\n",
    "    sig_sq = np.square(np.diag(sigma_slice))\n",
    "    prod = np.matmul(u_sq, sig_sq)\n",
    "    s = np.sqrt(np.sum(prod, axis = 1)).T\n",
    "\n",
    "    return s\n",
    "\n",
    "def get_important_sentences(u, sigma):\n",
    "    '''\n",
    "    Based on the sentence importance results, sort the indices to return indices that correspond to the\n",
    "    most importance sentence to least important\n",
    "\n",
    "    Args\n",
    "    - U, sigma matrices from SVD\n",
    "\n",
    "    Returns\n",
    "    - Vector of sentence indices in descending order of weight (1 x m)\n",
    "    '''\n",
    "\n",
    "    return (-weigh_sentence_importance(u, sigma)).argsort()\n",
    "\n",
    "def create_word_to_sentence_map(corpus):\n",
    "    '''\n",
    "    Creates a dictionary that maps a word from the vocab to all sentences with that word in the corpus.\n",
    "\n",
    "    Args\n",
    "    - corpus of sentences used in this summary\n",
    "\n",
    "    Returns\n",
    "    - the dictionary described\n",
    "    '''\n",
    "    \n",
    "    word_to_sentence = {}\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for i, doc in enumerate(corpus):\n",
    "        #remove punctuation while preserving contractions in text\n",
    "        sanitize_text = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokenized = word_tokenize(sanitize_text)\n",
    "        #remove duplicate words\n",
    "        tokenized = list(set([word.lower() for word in tokenized]))\n",
    "\n",
    "        for word in tokenized:\n",
    "            if word not in stop_words:\n",
    "                if word not in word_to_sentence:\n",
    "                    word_to_sentence[word] = [i]\n",
    "                else:\n",
    "                    word_to_sentence[word].append(i)\n",
    "    \n",
    "    return word_to_sentence\n",
    "\n",
    "def extract_summary(u, sigma, k, corpus):\n",
    "    '''\n",
    "    Helper method to get the text summary.\n",
    "\n",
    "    Summary will be taken from the top k sentences from getImportantSentences()\n",
    "    for each topic.\n",
    "\n",
    "    Args\n",
    "    - U, sigma from SVD\n",
    "    - k: number of sentences to include in summary\n",
    "    - corpus: the list of sentences\n",
    "\n",
    "    Returns\n",
    "    - the list of strings for the summary\n",
    "    '''\n",
    "\n",
    "    return [corpus[i] for i in get_important_sentences(u, sigma)[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(block_text):\n",
    "    '''\n",
    "    Preprocesses the original text to be summarized by tokenizing the sentences and removing\n",
    "    unnecessary characters.\n",
    "\n",
    "    Args\n",
    "    - block_text: text to be summarized\n",
    "\n",
    "    Returns\n",
    "    - list of sentences that can be used to create a summary\n",
    "    '''\n",
    "\n",
    "    tokenized = sent_tokenize(unidecode(block_text)) \n",
    "    return [token.replace('\\n',' ') for token in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "21"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "array([1., 1., 1., 1., 1., 1., 2., 1., 1., 2., 1., 1., 1., 1., 5., 6., 1.,\n       2., 2., 1., 1., 2., 5., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n       2., 5., 1., 1., 3., 1., 1., 1., 1., 1., 1., 4., 1., 2., 1., 1., 1.,\n       3., 1., 1., 1., 2., 1., 1., 1., 1., 2., 1., 1., 4., 1., 1., 1., 1.,\n       3., 9., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 9.,\n       1., 1., 1., 1., 1., 2., 4., 1., 2., 1., 2., 1., 2., 1., 1., 1., 2.,\n       1., 1., 1., 1., 1., 1., 1., 2., 1., 7., 3., 1., 2., 2., 1., 1., 1.,\n       1., 1., 2., 1., 1., 1., 1., 3., 1., 1., 1., 1., 1., 8., 7., 3., 1.,\n       1., 1., 2., 2., 1., 1., 1., 1., 1., 4., 1., 1., 2., 2., 1., 1., 1.,\n       1., 2., 1., 1., 4., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 3., 1., 1., 5., 1., 1., 1., 1., 1., 1., 1.])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "array([3.04452244, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 2.35137526, 3.04452244, 3.04452244, 2.35137526,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 1.43508453,\n       1.25276297, 3.04452244, 2.35137526, 2.35137526, 3.04452244,\n       3.04452244, 2.35137526, 1.43508453, 3.04452244, 3.04452244,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 2.35137526,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 2.35137526,\n       1.43508453, 3.04452244, 3.04452244, 1.94591015, 3.04452244,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       1.65822808, 3.04452244, 2.35137526, 3.04452244, 3.04452244,\n       3.04452244, 1.94591015, 3.04452244, 3.04452244, 3.04452244,\n       2.35137526, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       2.35137526, 3.04452244, 3.04452244, 1.65822808, 3.04452244,\n       3.04452244, 3.04452244, 3.04452244, 1.94591015, 0.84729786,\n       3.04452244, 2.35137526, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 2.35137526, 3.04452244, 3.04452244, 0.84729786,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       2.35137526, 1.65822808, 3.04452244, 2.35137526, 3.04452244,\n       2.35137526, 3.04452244, 2.35137526, 3.04452244, 3.04452244,\n       3.04452244, 2.35137526, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 2.35137526,\n       3.04452244, 1.09861229, 1.94591015, 3.04452244, 2.35137526,\n       2.35137526, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 2.35137526, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 1.94591015, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 3.04452244, 0.9650809 , 1.09861229, 1.94591015,\n       3.04452244, 3.04452244, 3.04452244, 2.35137526, 2.35137526,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       1.65822808, 3.04452244, 3.04452244, 2.35137526, 2.35137526,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 2.35137526,\n       3.04452244, 3.04452244, 1.65822808, 3.04452244, 3.04452244,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 1.94591015, 3.04452244, 3.04452244, 1.43508453,\n       3.04452244, 3.04452244, 3.04452244, 3.04452244, 3.04452244,\n       3.04452244, 3.04452244])"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "array([[0.        , 3.04452244, 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        3.04452244],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 3.04452244, 0.        ,\n        0.        ]])"
     },
     "metadata": {}
    }
   ],
   "source": [
    "text = '''\n",
    "The list of businesses impacted by a lockdown beginning Monday in Toronto and Peel Region were not clearly communicated, the owner of a Toronto massage spa says.\n",
    "\n",
    "While the Ontario government offered a partial list of what would remain open after the COVID-19 shutdown begins at 12:01 a.m., Kate Armstrong, owner and director of Bahn Thai Spa, told the Star she was unsure whether her business would be impacted.\n",
    "\n",
    "The Ontario government’s late-afternoon announcement on Friday stated that personal services, such as nail and hair salons, would now be closed. Missing, however, were details of all services included in the shutdown.\n",
    "\n",
    "However, on Sunday, the Ministry of Health confirmed to the Star that “regulated health professionals including dentists, optometrists, chiropractic services, ophthalmologists, physical and occupational therapists and podiatrists will be able to operate.”\n",
    "\n",
    "A spokesperson said that “under lockdown, regulated health professionals, including massage therapists, will be able to operate. Regulated health professionals such as registered massage therapists were not impacted and therefore not referenced.”\n",
    "\n",
    "Working “in partnership with the chief medical officer of health and our local medical officers of health, we continue to closely monitor the evolving situation to advise if and when public health measures need to be adjusted,” the spokesperson also said.\n",
    "\n",
    "In Ontario’s first lockdown last spring, physiotherapy, chiropractic services and massage therapists were among those to close their doors, which left some confused about what is happening this time around.\n",
    "\n",
    "“We have to continue to communicate with clients that are calling and saying, ‘Are we seeing you on Monday or not?’ We’re having to say we’ll call you as soon as we know something more,” Armstrong said.\n",
    "\n",
    "“It’s not like a haircut,” she said, adding that people are often seeking massage to treat physical pain or for mental health care.\n",
    "\n",
    "To Armstrong, massage has been as important as mental healthcare for Ontarians during the months-long pandemic. “I see the fatigue setting in on everyone’s faces ... The stress is so high … right now, (with) people not being able to be with their families. It’s so important to have human touch.”\n",
    "\n",
    "The Ontario Physiotherapy Association shared the news that physiotherapy services would be able to continue operations, said Shafiq Bhanji, president of Athlete’s Care Sports Medicine Centres.\n",
    "\n",
    "“We received direction from our respective colleges and professional associations on Friday and over the weekend via email indicating that our services would not be impacted the upcoming lockdown,” Bhanji said in an email to the Star.\n",
    "\n",
    "While Bhanji was able to confirm that Athlete’s Care could continue offering services and communicate that to clients via email, patients are still reaching out to verify whether they can keep their appointments.\n",
    "\n",
    "“It seems there was a fair bit of confusion in the general public about whether or not these services would be impacted,” Bhanji said. “... We are fortunate that our colleges and professional associations acted quickly to inform their members.”\n",
    "'''\n",
    "corpus = preprocess(text)\n",
    "\n",
    "tfidf_vec, vocab = tfidf(corpus)\n",
    "display(tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python_summarizerenv",
   "display_name": "Python_summarizerEnv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}