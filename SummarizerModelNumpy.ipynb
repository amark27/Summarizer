{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#download assets from nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def tfidf(corpus):\n",
    "    '''\n",
    "    Computes the TF-IDF (term frequency - inverse document frequency) matrix\n",
    "\n",
    "    Args\n",
    "    - corpus: a list of documents\n",
    "\n",
    "    Returns\n",
    "    - tfidfVec: an m x n matrix of the corpus. m = number of different terms used in the documents, n = number of documents \n",
    "    - vocab: all the unique words used in the corpus, excluding stop words\n",
    "    '''\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words = stopwords.words('english'))\n",
    "    tfidfVec = vectorizer.fit_transform(corpus)\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "        \n",
    "    return tfidfVec, vocab\n",
    "\n",
    "def freqCount(corpus):\n",
    "    pass\n",
    "\n",
    "def svd(tfidfVec):\n",
    "    '''\n",
    "    Gives the singular value decomposition of an m x n matrix.\n",
    "    A = U * sigma * V^t\n",
    "    \n",
    "    Args\n",
    "    - tfidfVec: an m x n matrix. m = number of documents or sentences, n = number of terms\n",
    "\n",
    "    Returns\n",
    "    - U: an m x r matrix of left singular values (document-topic table). r = number of topics\n",
    "    - sigma: an r x r diagonal matrix of singular values in decreasing order across the diagonal\n",
    "    - V^t: an n x r matrix of right singular values (term-topic table)\n",
    "    '''\n",
    "\n",
    "    lsa = TruncatedSVD(n_components = 10, n_iter=20)\n",
    "    u = lsa.fit_transform(tfidfVec)\n",
    "    sigma = lsa.singular_values_\n",
    "    vt = lsa.components_.T\n",
    "\n",
    "    return u, sigma, vt\n",
    "\n",
    "def getImportantSentences(u, sigma):\n",
    "    '''\n",
    "    Uses the LSA enhancement described by Josef Steinberg, et al.\n",
    "    Take all topics that have singular values > half of the largest singular value\n",
    "\n",
    "    Compute sk = sqrt(sum(v_ki^2 * sigma_i^2) from i = 1 to n)\n",
    "    sk is the length of the vector of the kth sentence\n",
    "    n is the number of topics \n",
    "\n",
    "    Args\n",
    "    - U, sigma matrices from SVD\n",
    "\n",
    "    Returns\n",
    "    - Vector of indices corresponding to the sentences in corpus sorted in descending order\n",
    "      of importance\n",
    "    '''\n",
    "\n",
    "    #look for the sigma value range that we need to consider using binary search\n",
    "    #sigma array is sorted in descending order and will never be empty\n",
    "    l, r, target = 0, len(sigma), sigma[0]/2\n",
    "    while l < r:\n",
    "        mid = l + (r-l)//2\n",
    "\n",
    "        if sigma[mid] < target:\n",
    "            r = mid\n",
    "        else:\n",
    "            l = mid + 1\n",
    "    sigmaBound = l\n",
    "\n",
    "    uSlice = u[:, :sigmaBound]\n",
    "    sigmaSlice = sigma[:sigmaBound]\n",
    "    uSq = np.square(uSlice)\n",
    "    sigSq = np.square(np.diag(sigmaSlice))\n",
    "    prod = np.matmul(uSq, sigSq)\n",
    "    result = np.sqrt(np.sum(prod, axis = 1)).T\n",
    "\n",
    "    return (-result).argsort()\n",
    "\n",
    "def createWordToSentenceMap(corpus):\n",
    "    '''\n",
    "    Creates a dictionary that maps a word from the vocab to all sentences with that word in the corpus.\n",
    "\n",
    "    Args\n",
    "    - corpus of sentences used in this summary\n",
    "\n",
    "    Returns\n",
    "    - the dictionary described\n",
    "    '''\n",
    "    \n",
    "    wordToSentence = {}\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "\n",
    "    for i, doc in enumerate(corpus):\n",
    "        #remove punctuation while preserving contractions in text\n",
    "        sanitizeText = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokenized = word_tokenize(sanitizeText)\n",
    "        #remove duplicate words\n",
    "        tokenized = list(set([word.lower() for word in tokenized]))\n",
    "\n",
    "        for word in tokenized:\n",
    "            if word not in stopWords:\n",
    "                if word not in wordToSentence:\n",
    "                    wordToSentence[word] = [i]\n",
    "                else:\n",
    "                    wordToSentence[word].append(i)\n",
    "    \n",
    "    return wordToSentence\n",
    "\n",
    "def extractSummary(u, sigma, k, corpus):\n",
    "    '''\n",
    "    Helper method to get the text summary.\n",
    "\n",
    "    Summary will be taken from the top k sentences from getImportantSentences()\n",
    "    for each topic.\n",
    "\n",
    "    Args\n",
    "    - U, sigma from SVD\n",
    "    - k: number of sentences to include in summary\n",
    "    - corpus: the list of sentences\n",
    "    '''\n",
    "\n",
    "    return [corpus[i] for i in getImportantSentences(u, sigma)[:k]]"
   ]
  }
 ]
}